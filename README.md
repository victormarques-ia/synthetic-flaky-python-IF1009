# Synthetic Flaky Python

A **comprehensive framework** for empirical study of flaky test mitigation techniques, developed for academic research.

## ğŸ“‹ Overview

This project implements a controlled benchmark to study different types of flakiness and mitigation strategies in automated tests. The framework provides:

- **Systematic Classification** of 5 main flakiness types with controlled reproduction
- **Empirical Evaluation** of 4 mitigation techniques with quantitative metrics  
- **Cost-Benefit Analysis** of each technique with ROI calculations
- **Practical Recommendations** for developers based on data-driven insights
- **Reproducible Framework** with fixed seeds for consistent results

## ğŸ—ï¸ Project Structure

```
synthetic-flaky-python/
â”œâ”€â”€ tests/                          # Comprehensive test suite (5 flakiness types)
â”‚   â”œâ”€â”€ test_stable.py              # stable tests (baseline)
â”‚   â”œâ”€â”€ test_flaky_randomness.py    # randomness-based flaky tests  
â”‚   â”œâ”€â”€ test_flaky_race.py          # race condition tests
â”‚   â”œâ”€â”€ test_flaky_order.py         # order dependency tests
â”‚   â”œâ”€â”€ test_flaky_external.py      # external dependency tests
â”‚   â”œâ”€â”€ test_flaky_timeout.py       # timeout-sensitive tests
â”‚   â””â”€â”€ pytest.ini                  # pytest configuration
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_comprehensive_study.py  # ğŸ†• MAIN: orchestrator script
â”‚   â”œâ”€â”€ config/                     # Configuration management
â”‚   â”‚   â””â”€â”€ study_config.py         # Study settings and flakiness profiles
â”‚   â”œâ”€â”€ classification/             # Flakiness classification system
â”‚   â”‚   â””â”€â”€ flakiness_classifier.py # Systematic type classification
â”‚   â”œâ”€â”€ execution/                  # Test execution engines
â”‚   â”‚   â””â”€â”€ experiment_runner.py    # Baseline and mitigation runners
â”‚   â”œâ”€â”€ analysis/                   # Data analysis modules
â”‚   â”‚   â””â”€â”€ data_analyzer.py        # Statistical analysis and metrics
â”‚   â”œâ”€â”€ visualization/              # Chart generation
â”‚   â”‚   â””â”€â”€ chart_generator.py      # Publication-quality visualizations
â”‚   â”œâ”€â”€ reporting/                  # Report generation
â”‚   â”‚   â””â”€â”€ report_generator.py     # Text reports and data persistence
â”‚   â”œâ”€â”€ utils/                      # Shared utilities
â”‚   â”‚   â””â”€â”€ helpers.py              # Common helper functions
â”‚   â”œâ”€â”€ MODULE_STRUCTURE.md         # ğŸ“‹ Detailed module documentation
â”œâ”€â”€ comprehensive_results/          # Complete study results
â””â”€â”€ README.md
```

## ğŸ›ï¸ Modular Architecture

The framework uses a **modular design** for maintainability and extensibility:

- **ğŸ¯ `run_comprehensive_study.py`**: Main orchestrator script
- **âš™ï¸ `config/`**: Centralized configuration and flakiness type definitions
- **ğŸ”¬ `classification/`**: Systematic flakiness classification system  
- **âš¡ `execution/`**: Test execution engines for baseline and mitigation
- **ğŸ“Š `analysis/`**: Statistical analysis and effectiveness metrics
- **ğŸ“ˆ `visualization/`**: Publication-quality chart generation
- **ğŸ“‹ `reporting/`**: Report generation and data persistence
- **ğŸ› ï¸ `utils/`**: Shared helper functions and utilities

ğŸ“‹ **Detailed Architecture**: See [`scripts/MODULE_STRUCTURE.md`](scripts/MODULE_STRUCTURE.md) for complete module documentation.

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone the repository
git clone <repository-url>
cd synthetic-flaky-python

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies (includes mitigation packages)
pip install -r requirements.txt
```

### 2. Run Complete Study

```bash
# ğŸ¯ COMPREHENSIVE STUDY (RECOMMENDED)
# Executes baseline analysis + mitigation strategies + complete analysis
python scripts/run_comprehensive_study.py

# Quick test (for development/testing)
python scripts/run_comprehensive_study.py --baseline-runs 2 --mitigation-runs 2

# With custom settings
python scripts/run_comprehensive_study.py --baseline-runs 10 --mitigation-runs 10 --verbose
```

### 3. Results Generated

```bash
# All results saved in comprehensive_results/
# âœ… comprehensive_study_report.txt     - Complete analysis report
# âœ… comprehensive_study_data.json      - All raw data and results  
# âœ… *.png                              - 4 visualization charts
# âœ… *.csv                              - Summary tables for analysis
```

## ğŸ§ª Flakiness Classification System

The framework systematically implements **5 main categories** of flakiness for empirical analysis:

### 1. **Randomness**
Tests that depend on random numbers or probability.

**Example:**
```python
def test_random_coin_flip():
    """Fails ~50% of the time"""
    assert random.random() > 0.5
```

### 2. **Race Conditions**
Race conditions between threads and concurrency issues.

**Example:**
```python
def test_race_counter_increment():
    """Race condition in shared counter"""
    # Two threads incrementing simultaneously
    assert shared_counter["value"] == 20  # May fail
```

### 3. **Order Dependencies**
Tests that depend on execution order or global state.

**Example:**
```python
def test_order_use_state():
    """Depends on test_order_initialize running first"""
    assert global_state["initialized"] is True
```

### 4. **External Dependencies** 
Simulation of unstable APIs, databases, and external services.

**Example:**
```python
def test_external_api_call():
    """Simulates API that may fail (30% chance)"""
    response = MockExternalAPI.get_user_data(123)
    assert response["id"] == 123
```

### 5. **Timeout Issues**
Tests sensitive to performance and time limitations.

**Example:**
```python
def test_timeout_deadline_sensitive():
    """Fails if processing takes too long"""
    duration = measure_processing_time()
    assert duration < 0.02  # 20ms limit
```

## ğŸ› ï¸ Mitigation Strategies

The framework evaluates **4 main mitigation techniques**:

### 1. **Retries** (pytest-rerunfailures)
Re-execute failed tests multiple times to handle transient failures.

**Configuration:**
```bash
pytest --reruns=3 --reruns-delay=1
```

**Best for:** Timeout issues, external dependencies
**Pros:** Easy to implement, low maintenance
**Cons:** Increases execution time, doesn't fix root cause

### 2. **Mocking** (unittest.mock)
Replace unreliable dependencies with predictable mock objects.

**Example:**
```python
@patch('random.random')
def test_with_mock(mock_random):
    mock_random.return_value = 0.8  # Deterministic
    assert random.random() > 0.5    # Always passes
```

**Best for:** External dependencies, randomness
**Pros:** Eliminates external failures, fast execution
**Cons:** High maintenance, may miss integration issues

### 3. **Test Isolation** (pytest-forked)
Execute each test in a separate process to prevent state sharing.

**Configuration:**
```bash
pytest --forked
```

**Best for:** Order dependencies, race conditions
**Pros:** Prevents state contamination, reliable
**Cons:** Slower execution, higher resource usage

### 4. **Combined Strategy**
Use multiple techniques together for maximum effectiveness.

**Configuration:**
```bash
pytest --reruns=2 --forked  # + mocking
```

**Best for:** Complex systems with multiple flakiness types
**Pros:** Highest effectiveness
**Cons:** Most complex and expensive to implement

## ğŸ“Š Comprehensive Study Design

The framework executes a **two-phase empirical study**:

### **Phase 1: Baseline Analysis**
Tests each flakiness type separately to establish baseline characteristics:

### **Phase 2: Mitigation Analysis**
Tests each mitigation strategy against all flaky tests to measure effectiveness.

## ğŸ“ˆ Comprehensive Metrics

### **Flakiness Classification Metrics**
- **Pass Rate**: Proportion of successful test executions
- **Flakiness Index**: Coefficient of variation in pass rates (instability measure)
- **Severity Classification**: Low/Moderate/High/Severe based on index
- **Predictability**: Consistency of failure patterns

### **Mitigation Effectiveness Metrics**
- **Improvement Rate**: Relative % improvement in pass rates
- **Performance Overhead**: % increase in execution time
- **Effectiveness Score**: Weighted score considering improvement vs overhead
- **Cost-Effectiveness Ratio**: Improvement per unit of overhead

### **Cost-Benefit Analysis**
- **Implementation Cost**: Relative complexity to implement (1-10 scale)
- **Maintenance Cost**: Ongoing effort required (1-10 scale)
- **Return on Investment (ROI)**: (Benefits - Costs) / Costs
- **Strategy Ranking**: Ordered by ROI and effectiveness

### **Generated Outputs**
- **ğŸ“„ comprehensive_study_report.txt**: Complete analysis with recommendations
- **ğŸ“Š comprehensive_study_data.json**: All raw data and calculated metrics
- **ğŸ“ˆ Visualizations**: 4 charts covering all aspects of the study
- **ğŸ“‹ CSV Summaries**: Baseline and mitigation data for further analysis

## ğŸ“‹ Example Study Results

```
ğŸ‰ COMPREHENSIVE STUDY COMPLETED!
======================================================================
ğŸ“Š Study Duration: 12.3 minutes
ğŸ“ Results Directory: comprehensive_results/

ğŸ† KEY FINDINGS:
   Best ROI: RETRIES (ROI: 2.45)
   Most Effective: COMBINED (Score: 0.847)
   Most Problematic Type: Randomness (47.2% pass rate)

FLAKINESS CLASSIFICATION ANALYSIS
----------------------------------------
RANDOMNESS:
  â€¢ Pass Rate: 47.2% (flakiness index: 0.342)
  â€¢ Severity: High
  â€¢ Mechanism: Non-deterministic assertions based on random values

EXTERNAL:
  â€¢ Pass Rate: 68.5% (flakiness index: 0.198)
  â€¢ Severity: Moderate  
  â€¢ Mechanism: Network failures and service unavailability

MITIGATION STRATEGY EFFECTIVENESS
----------------------------------------
RETRIES:
  â€¢ Pass Rate Improvement: +23.4%
  â€¢ Performance Overhead: +47.2%
  â€¢ Effectiveness Score: 0.634

MOCKING:
  â€¢ Pass Rate Improvement: +41.8%
  â€¢ Performance Overhead: +12.1%
  â€¢ Effectiveness Score: 0.798

PRACTICAL RECOMMENDATIONS
----------------------------------------
Implementation Priority Ranking:
  1. MOCKING (ROI: 3.12)
  2. RETRIES (ROI: 2.45)
  3. ISOLATION (ROI: 1.89)
  4. COMBINED (ROI: 1.34)

Recommendations by Flakiness Type:
  â€¢ Randomness: Use MOCKING (Expected effectiveness: 90%)
  â€¢ External: Use MOCKING (Expected effectiveness: 95%)
  â€¢ Race: Use ISOLATION (Expected effectiveness: 90%)
```

## âš™ï¸ Configuration Options

### Comprehensive Study Script (`run_comprehensive_study.py`)

```bash
python scripts/run_comprehensive_study.py [options]

Options:
  --baseline-runs N     Runs per baseline type (default: 15)
  --mitigation-runs N   Runs per mitigation strategy (default: 15)  
  --output-dir DIR      Output directory (default: comprehensive_results)
  --seeds N N N         Random seeds for reproducibility (default: 42 123 999)
  --verbose            Detailed output during execution
  --skip-baseline      Skip baseline phase (use existing data)
  --skip-mitigation    Skip mitigation phase (use existing data)
```

## ğŸ“ For Academic Research

### **Publication-Ready Study**
```bash
# Complete empirical study for papers/thesis
python scripts/run_comprehensive_study.py --baseline-runs 30 --mitigation-runs 10
```

### **Generated Outputs**
- **ğŸ“„ Comprehensive Report**
- **ğŸ“Š Quantitative Data**
- **ğŸ“ˆ Publication-Quality Visualizations**
- **ğŸ¯ Evidence-Based Recommendations**
- **ğŸ“‹ Systematic Classification**
- **ğŸ’° Cost-Benefit Analysis**

### **Research Contributions**
- **Techniques**: Reproducible framework with controlled flakiness
- **Practices**: Systematic evaluation protocol for mitigation strategies  
- **Results**: Quantitative effectiveness data and practical recommendations

## ğŸ”¬ Framework Extension

### **Adding New Flakiness Types**
1. Create new test file: `tests/test_flaky_newtype.py`
2. Add appropriate markers: `@pytest.mark.flaky`, `@pytest.mark.newtype`
3. Update flakiness profiles in `scripts/classification/flakiness_classifier.py`:

```python
"newtype": FlakynessProfile(
    test_type="newtype",
    description="Your new flakiness description",
    failure_mechanism="Explanation of failure",
    typical_pass_rate=0.6,  # Expected pass rate
    mitigation_effectiveness={
        "retries": 0.3, "mocking": 0.8, 
        "isolation": 0.5, "combined": 0.9
    }
)
```

### **Adding New Mitigation Strategies**
1. Implement strategy method in `scripts/execution/experiment_runner.py`
2. Add to `MitigationRunner` class strategies
3. Update cost definitions in `scripts/analysis/data_analyzer.py`
4. Update visualization logic in `scripts/visualization/chart_generator.py`

## ğŸ“š Data Structure

### Comprehensive Study Data (`comprehensive_study_data.json`)
```json
{
  "study_metadata": {
    "study_type": "comprehensive_flaky_test_analysis",
    "timestamp": "2024-01-01T12:00:00",
    "configuration": { "baseline_runs": 10, "mitigation_runs": 10 }
  },
  "baseline_results": {
    "randomness": {
      "avg_pass_rate": 0.472,
      "flakiness_index": 0.342,
      "total_runs": 90
    }
  },
  "mitigation_results": {
    "retries": {
      "avg_pass_rate": 0.756,
      "avg_execution_time": 2.34,
      "total_runs": 10
    }
  },
  "analysis_results": {
    "mitigation_effectiveness": {
      "retries": {
        "pass_rate_improvement": { "relative_percent": 23.4 },
        "effectiveness_score": 0.634
      }
    },
    "cost_benefit_analysis": {
      "retries": { "roi": 2.45, "recommendation": "Recommended" }
    }
  }
}
```

### CSV Summaries
- **`baseline_summary.csv`**: Pass rates and flakiness indices by type
- **`mitigation_summary.csv`**: Effectiveness and overhead by strategy


## ğŸ¤ Contributing

1. Fork the project
2. Create feature branch (`git checkout -b feature/new-flaky-type`)
3. Commit changes (`git commit -am 'Add timeout flaky'`)
4. Push to branch (`git push origin feature/new-flaky-type`)
5. Open Pull Request

## ğŸ“„ License

MIT License - see LICENSE file for details.